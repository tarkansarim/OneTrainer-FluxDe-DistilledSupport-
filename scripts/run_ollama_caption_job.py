#!/usr/bin/env python
"""
Standalone multi-GPU detail crop caption runner.

Reads a manifest generated by CropCaptionGenerator, spins up one Ollama instance per GPU,
and processes each GPU's batch in its own cmd window. All noisy logging stays in those
windows or the per-run log file so the main OneTrainer console stays quiet.
"""

import sys
import os

# Immediate logging before any other imports to catch early failures
print(f"[CaptionJob] Script starting - PID: {os.getpid()}", file=sys.stderr, flush=True)
print(f"[CaptionJob] Python: {sys.executable}", file=sys.stderr, flush=True)
print(f"[CaptionJob] Python version: {sys.version}", file=sys.stderr, flush=True)

import argparse
import base64
import io
import json
import math
import signal
import subprocess
import time
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path
from typing import Any, Dict, List, Sequence

print(f"[CaptionJob] Basic imports completed", file=sys.stderr, flush=True)

import requests
from PIL import Image

print(f"[CaptionJob] All imports completed", file=sys.stderr, flush=True)

# Set up signal handler to catch SIGTERM
def signal_handler(signum, frame):
    print(f"[CaptionJob] Received signal {signum} (SIGTERM)", file=sys.stderr, flush=True)
    import traceback
    print(f"[CaptionJob] Stack trace at signal:", file=sys.stderr, flush=True)
    traceback.print_stack(frame, file=sys.stderr)
    sys.stderr.flush()
    sys.exit(128 + signum)

signal.signal(signal.SIGTERM, signal_handler)
print(f"[CaptionJob] Signal handler installed", file=sys.stderr, flush=True)

REPO_ROOT = Path(__file__).resolve().parents[1]
if str(REPO_ROOT) not in sys.path:
    sys.path.append(str(REPO_ROOT))

print(f"[CaptionJob] About to import MultiOllamaManager", file=sys.stderr, flush=True)
from modules.util.ollama_multi_manager import MultiOllamaManager
print(f"[CaptionJob] MultiOllamaManager imported successfully", file=sys.stderr, flush=True)


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Run multi-GPU Ollama captioning for detail crops.")
    parser.add_argument(
        "--manifest",
        required=True,
        help="Path to the manifest JSON generated by CropCaptionGenerator.",
    )
    parser.add_argument(
        "--max-image-side",
        type=int,
        default=512,
        help="Resize longer side to this many pixels before encoding (default: 512).",
    )
    return parser.parse_args()


def load_manifest(path: str) -> Dict[str, Any]:
    with open(path, "r", encoding="utf-8") as fh:
        return json.load(fh)


def kill_all_ollama_processes() -> None:
    """Kill all Ollama processes using non-blocking approach to avoid hanging."""
    if os.name == "nt":
        commands = [
            ["sc", "stop", "Ollama"],
            ["taskkill", "/IM", "ollama.exe", "/F", "/T"],
            ["taskkill", "/IM", "ollama app.exe", "/F", "/T"],
        ]
    else:
        # First check if any ollama serve processes exist before trying to kill
        # This avoids unnecessary pkill calls that might cause issues
        try:
            check_result = subprocess.run(
                ["pgrep", "-f", "ollama serve"],
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL,
                timeout=1.0,
            )
            if check_result.returncode != 0:
                # No ollama processes found, nothing to kill
                return
        except Exception:
            # If check fails, proceed with kill attempt anyway
            pass
        
        # Use pkill with more specific pattern to avoid matching ourselves
        # Only match processes with "ollama serve" in the command line
        commands = [["pkill", "-f", "ollama serve"]]
    
    for cmd in commands:
        try:
            # Use Popen with immediate return to avoid blocking
            # Don't wait for completion - fire and forget
            # start_new_session detaches from parent to avoid signal propagation
            subprocess.Popen(
                cmd,
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL,
                start_new_session=True,
            )
            # Don't store or wait for process - just fire and forget
        except Exception:
            # Any exception is fine - we're just trying to clean up
            pass


def ensure_model_available(model: str, host: str = None, timeout: float = 300.0) -> None:
    if not model:
        return
    try:
        env = os.environ.copy()
        if host:
            # Extract host:port from URL (e.g., "http://127.0.0.1:12134" -> "127.0.0.1:12134")
            if host.startswith("http://"):
                host = host[7:]
            elif host.startswith("https://"):
                host = host[8:]
            env["OLLAMA_HOST"] = host
        result = subprocess.run(
            ["ollama", "pull", model],
            check=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            timeout=timeout,
            text=True,
            env=env,
        )
        # Log success (but only if there's output, to avoid spam)
        if result.stdout:
            print(f"[CaptionJob] Model '{model}' pulled successfully", flush=True)
    except subprocess.CalledProcessError as exc:
        # Capture the actual error output (already text since text=True)
        stderr_output = exc.stderr if exc.stderr else "No error output captured"
        stdout_output = exc.stdout if exc.stdout else ""
        error_msg = f"ollama pull failed for model '{model}'. Exit code {exc.returncode}."
        if stderr_output and stderr_output.strip():
            error_msg += f"\nError output: {stderr_output.strip()}"
        if stdout_output and stdout_output.strip():
            error_msg += f"\nOutput: {stdout_output.strip()}"
        raise RuntimeError(error_msg) from exc
    except subprocess.TimeoutExpired as exc:
        raise RuntimeError(f"ollama pull timed out after {timeout} seconds for model '{model}'") from exc
    except FileNotFoundError as exc:
        raise RuntimeError("Unable to locate the 'ollama' CLI. Install Ollama or add it to PATH.") from exc


def chunk_jobs(jobs: List[Dict[str, Any]], buckets: int) -> List[List[Dict[str, Any]]]:
    if buckets <= 0:
        return [jobs]
    chunk_size = max(1, math.ceil(len(jobs) / buckets))
    result: List[List[Dict[str, Any]]] = []
    for i in range(buckets):
        start = i * chunk_size
        end = start + chunk_size
        result.append(jobs[start:end])
    return result


def encode_image(path: str, max_side: int) -> str:
    with Image.open(path) as img:
        rgb = img.convert("RGB")
        if max_side and max(rgb.size) > max_side:
            resample_attr = getattr(Image, "Resampling", Image)
            rgb.thumbnail((max_side, max_side), resample_attr.LANCZOS)
        buffer = io.BytesIO()
        rgb.save(buffer, format="PNG")
    return base64.b64encode(buffer.getvalue()).decode("utf-8")


def sanitize_caption(value: str) -> str:
    if not value:
        return ""
    cleaned = value.strip()
    if "unanswerable" in cleaned.lower():
        return ""
    return cleaned


def write_caption_file(path: str, caption: str) -> None:
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8") as fh:
        fh.write(caption)


def generate_caption(session: requests.Session, host: str, job: Dict[str, Any], max_side: int) -> str:
    image_b64 = encode_image(job["image_path"], max_side)
    payload = {
        "model": job["model"],
        "system": job.get("system_prompt", ""),
        "prompt": job["prompt"],
        "images": [image_b64],
        "stream": False,
        "options": {
            "temperature": 0.0,
            "num_predict": int(job.get("max_tokens", 256)),
        },
    }

    timeout = float(job.get("timeout", 120.0) or 120.0)
    max_retries = int(job.get("max_retries", 4) or 4)
    backoff = 5.0
    attempt = 0
    last_error: Exception | None = None
    url = f"{host.rstrip('/')}/api/generate"

    while attempt <= max_retries:
        try:
            response = session.post(url, json=payload, timeout=timeout)
            if response.status_code != 200:
                raise RuntimeError(f"Ollama responded with status {response.status_code}: {response.text}")
            data = response.json()
            caption = sanitize_caption(str(data.get("response", "")).strip())
            return caption
        except Exception as exc:
            last_error = exc
            attempt += 1
            if attempt > max_retries:
                break
            time.sleep(min(backoff, 30.0))
            backoff *= 2.0
    raise RuntimeError(f"Failed to caption '{job['image_path']}' after {max_retries + 1} attempts: {last_error}")


def process_bucket(host: str, jobs: Sequence[Dict[str, Any]], max_side: int, worker_id: int) -> None:
    session = requests.Session()
    total = len(jobs)
    for idx, job in enumerate(jobs, start=1):
        caption = generate_caption(session, host, job, max_side)
        write_caption_file(job["caption_path"], caption)
        print(
            f"[GPU worker {worker_id}] {idx}/{total} -> {os.path.basename(job['caption_path'])}",
            flush=True,
        )


def main() -> int:
    print(f"[CaptionJob] Starting caption job (Python {sys.version.split()[0]})", flush=True)
    print(f"[CaptionJob] Script location: {Path(__file__).resolve()}", flush=True)
    args = parse_args()
    print(f"[CaptionJob] Loading manifest: {args.manifest}", flush=True)
    manifest = load_manifest(args.manifest)
    jobs: List[Dict[str, Any]] = manifest.get("jobs", [])
    if not jobs:
        print("[CaptionJob] Manifest contained zero jobs; nothing to do.")
        return 0

    config = manifest.get("config", {})
    gpu_indices = config.get("gpu_indices") or [0]
    base_port = int(config.get("base_port", 12134))
    show_console = bool(config.get("show_console", True))

    print(f"[CaptionJob] Cleaning up any existing Ollama processes...", flush=True)
    try:
        kill_all_ollama_processes()
        print(f"[CaptionJob] Ollama cleanup initiated (non-blocking)", flush=True)
    except Exception as e:
        print(f"[CaptionJob] Warning: Ollama cleanup failed (non-fatal): {e}", flush=True)
    except KeyboardInterrupt:
        # If we're interrupted during cleanup, just continue
        print(f"[CaptionJob] Cleanup interrupted, continuing anyway", flush=True)
    
    manager = MultiOllamaManager(gpu_indices, base_port=base_port, show_console=show_console)
    hosts: List[str] = []
    try:
        manager.start_all()
        hosts = manager.get_hosts()
        if not hosts:
            raise RuntimeError("No Ollama hosts were started; check Ollama installation and logs.")
        
        # Pull models AFTER Ollama servers are started (they need a server to connect to)
        # Use the first host to pull models (models are shared across all servers)
        unique_models = sorted({job.get("model") for job in jobs if job.get("model")})
        for model in unique_models:
            ensure_model_available(model, host=hosts[0] if hosts else None)
        buckets = chunk_jobs(jobs, len(hosts))

        with ThreadPoolExecutor(max_workers=len(hosts)) as executor:
            futures = []
            for idx, host in enumerate(hosts):
                bucket = buckets[idx] if idx < len(buckets) else []
                futures.append(executor.submit(process_bucket, host, bucket, args.max_image_side, idx))
            for future in futures:
                future.result()
    finally:
        manager.stop_all(force=True)
        kill_all_ollama_processes()

    print("[CaptionJob] All GPU workers completed successfully.")
    return 0


if __name__ == "__main__":
    try:
        exit_code = main()
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("[CaptionJob] Interrupted by user", file=sys.stderr)
        sys.exit(130)
    except Exception as exc:
        import traceback
        error_msg = f"[CaptionJob] FATAL ERROR: {type(exc).__name__}: {exc}\n"
        error_msg += "".join(traceback.format_exception(type(exc), exc, exc.__traceback__))
        print(error_msg, file=sys.stderr)
        sys.stderr.flush()
        sys.exit(1)

