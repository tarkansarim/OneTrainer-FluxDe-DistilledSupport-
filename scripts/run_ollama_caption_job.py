#!/usr/bin/env python
"""
Standalone multi-GPU detail crop caption runner.

Reads a manifest generated by CropCaptionGenerator, spins up one Ollama instance per GPU,
and processes each GPU's batch in its own cmd window. All noisy logging stays in those
windows or the per-run log file so the main OneTrainer console stays quiet.
"""

import argparse
import base64
import io
import json
import math
import os
import subprocess
import sys
import time
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path
from typing import Any, Dict, List, Sequence

import requests
from PIL import Image

REPO_ROOT = Path(__file__).resolve().parents[1]
if str(REPO_ROOT) not in sys.path:
    sys.path.append(str(REPO_ROOT))

from modules.util.ollama_multi_manager import MultiOllamaManager


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Run multi-GPU Ollama captioning for detail crops.")
    parser.add_argument(
        "--manifest",
        required=True,
        help="Path to the manifest JSON generated by CropCaptionGenerator.",
    )
    parser.add_argument(
        "--max-image-side",
        type=int,
        default=512,
        help="Resize longer side to this many pixels before encoding (default: 512).",
    )
    return parser.parse_args()


def load_manifest(path: str) -> Dict[str, Any]:
    with open(path, "r", encoding="utf-8") as fh:
        return json.load(fh)


def kill_all_ollama_processes() -> None:
    if os.name == "nt":
        commands = [
            ["sc", "stop", "Ollama"],
            ["taskkill", "/IM", "ollama.exe", "/F", "/T"],
            ["taskkill", "/IM", "ollama app.exe", "/F", "/T"],
        ]
    else:
        commands = [["pkill", "-f", "ollama"]]
    for cmd in commands:
        try:
            subprocess.run(cmd, check=False, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        except Exception:
            pass


def ensure_model_available(model: str, timeout: float = 300.0) -> None:
    if not model:
        return
    try:
        subprocess.run(
            ["ollama", "pull", model],
            check=True,
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL,
            timeout=timeout,
        )
    except subprocess.CalledProcessError as exc:
        raise RuntimeError(f"ollama pull failed for model '{model}'. Exit code {exc.returncode}.") from exc
    except FileNotFoundError as exc:
        raise RuntimeError("Unable to locate the 'ollama' CLI. Install Ollama or add it to PATH.") from exc


def chunk_jobs(jobs: List[Dict[str, Any]], buckets: int) -> List[List[Dict[str, Any]]]:
    if buckets <= 0:
        return [jobs]
    chunk_size = max(1, math.ceil(len(jobs) / buckets))
    result: List[List[Dict[str, Any]]] = []
    for i in range(buckets):
        start = i * chunk_size
        end = start + chunk_size
        result.append(jobs[start:end])
    return result


def encode_image(path: str, max_side: int) -> str:
    with Image.open(path) as img:
        rgb = img.convert("RGB")
        if max_side and max(rgb.size) > max_side:
            resample_attr = getattr(Image, "Resampling", Image)
            rgb.thumbnail((max_side, max_side), resample_attr.LANCZOS)
        buffer = io.BytesIO()
        rgb.save(buffer, format="PNG")
    return base64.b64encode(buffer.getvalue()).decode("utf-8")


def sanitize_caption(value: str) -> str:
    if not value:
        return ""
    cleaned = value.strip()
    if "unanswerable" in cleaned.lower():
        return ""
    return cleaned


def write_caption_file(path: str, caption: str) -> None:
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8") as fh:
        fh.write(caption)


def generate_caption(session: requests.Session, host: str, job: Dict[str, Any], max_side: int) -> str:
    image_b64 = encode_image(job["image_path"], max_side)
    payload = {
        "model": job["model"],
        "system": job.get("system_prompt", ""),
        "prompt": job["prompt"],
        "images": [image_b64],
        "stream": False,
        "options": {
            "temperature": 0.0,
            "num_predict": int(job.get("max_tokens", 256)),
        },
    }

    timeout = float(job.get("timeout", 120.0) or 120.0)
    max_retries = int(job.get("max_retries", 4) or 4)
    backoff = 5.0
    attempt = 0
    last_error: Exception | None = None
    url = f"{host.rstrip('/')}/api/generate"

    while attempt <= max_retries:
        try:
            response = session.post(url, json=payload, timeout=timeout)
            if response.status_code != 200:
                raise RuntimeError(f"Ollama responded with status {response.status_code}: {response.text}")
            data = response.json()
            caption = sanitize_caption(str(data.get("response", "")).strip())
            return caption
        except Exception as exc:
            last_error = exc
            attempt += 1
            if attempt > max_retries:
                break
            time.sleep(min(backoff, 30.0))
            backoff *= 2.0
    raise RuntimeError(f"Failed to caption '{job['image_path']}' after {max_retries + 1} attempts: {last_error}")


def process_bucket(host: str, jobs: Sequence[Dict[str, Any]], max_side: int, worker_id: int) -> None:
    session = requests.Session()
    total = len(jobs)
    for idx, job in enumerate(jobs, start=1):
        caption = generate_caption(session, host, job, max_side)
        write_caption_file(job["caption_path"], caption)
        print(
            f"[GPU worker {worker_id}] {idx}/{total} -> {os.path.basename(job['caption_path'])}",
            flush=True,
        )


def main() -> int:
    args = parse_args()
    manifest = load_manifest(args.manifest)
    jobs: List[Dict[str, Any]] = manifest.get("jobs", [])
    if not jobs:
        print("[CaptionJob] Manifest contained zero jobs; nothing to do.")
        return 0

    config = manifest.get("config", {})
    gpu_indices = config.get("gpu_indices") or [0]
    base_port = int(config.get("base_port", 12134))
    show_console = bool(config.get("show_console", True))

    kill_all_ollama_processes()
    unique_models = sorted({job.get("model") for job in jobs if job.get("model")})
    for model in unique_models:
        ensure_model_available(model)

    manager = MultiOllamaManager(gpu_indices, base_port=base_port, show_console=show_console)
    hosts: List[str] = []
    try:
        manager.start_all()
        hosts = manager.get_hosts()
        if not hosts:
            raise RuntimeError("No Ollama hosts were started; check Ollama installation and logs.")
        buckets = chunk_jobs(jobs, len(hosts))

        with ThreadPoolExecutor(max_workers=len(hosts)) as executor:
            futures = []
            for idx, host in enumerate(hosts):
                bucket = buckets[idx] if idx < len(buckets) else []
                futures.append(executor.submit(process_bucket, host, bucket, args.max_image_side, idx))
            for future in futures:
                future.result()
    finally:
        manager.stop_all(force=True)
        kill_all_ollama_processes()

    print("[CaptionJob] All GPU workers completed successfully.")
    return 0


if __name__ == "__main__":
    sys.exit(main())

